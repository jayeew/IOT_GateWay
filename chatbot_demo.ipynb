{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiayiwang5/IOT_GateWay/blob/master/chatbot_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BPOsyEiC1MVI",
        "colab_type": "code",
        "outputId": "7c2bc573-0ece-4411-8ed9-ce8c48b6092f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "data_dir = np.arange(0, 3000, 1000)\n",
        "main_path = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "def get_file_list(file_path):\n",
        "    dir_list = os.listdir(file_path)\n",
        "    if not dir_list:\n",
        "        return\n",
        "    else:\n",
        "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
        "    return dir_list\n",
        "with open(main_path + 'middle_data/dictionary.pkl', 'rb') as f:\n",
        "    word_to_index = pickle.load(f)\n",
        "for i, j in word_to_index.items():\n",
        "      word_to_index[i] = j + 1\n",
        "index_to_word = {}\n",
        "for key, value in word_to_index.items():\n",
        "    index_to_word[value] = key\n",
        "with open(main_path + 'middle_data/words.pkl', 'rb') as f:\n",
        "    words = pickle.load(f)\n",
        "maxLen = 20\n",
        "vocab_size = len(word_to_index) + 1\n",
        "print('word_to_vec_map: ', len(list(words)))\n",
        "print('vocab_size: ', vocab_size)\n",
        "with open(main_path + 'middle_data/embedding_matrix.pkl', 'rb') as f:\n",
        "      embedding_matrix = pickle.load(f)\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_to_vec_map:  400000\n",
            "vocab_size:  42905\n",
            "(42905, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GqcLHzpro5Tb",
        "colab_type": "code",
        "outputId": "7bb9a3ec-3a6c-4629-d865-c8ec8461c772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "def generate_train(batch_size):\n",
        "    print('\\n*********************************generate_train()*********************************')\n",
        "    steps=0\n",
        "    context_ = np.load(main_path + 'middle_data/context_indexes.npy')\n",
        "    final_target_ = np.load(main_path + 'middle_data/target_indexes.npy')\n",
        "    context_ = context_[:90000]\n",
        "    final_target_ = final_target_[:90000]\n",
        "    while True:\n",
        "        context = context_[steps:steps+batch_size]\n",
        "        final_target = final_target_[steps:steps+batch_size]\n",
        "        for pos, i in enumerate(final_target):\n",
        "            for pos_, j in enumerate(i):\n",
        "                i[pos_] = j + 1\n",
        "            if(len(i) > maxLen):\n",
        "                final_target[pos] = i[:maxLen]\n",
        "\n",
        "        for pos, i in enumerate(context):\n",
        "            for pos_, j in enumerate(i):\n",
        "                i[pos_] = j + 1\n",
        "            if(len(i) > maxLen):\n",
        "                context[pos] = i[:maxLen]\n",
        "#         print('\\ncontext.shape: ', context.shape)\n",
        "        outs = np.zeros([context.shape[0], maxLen, vocab_size], dtype='float32')\n",
        "        for pos, i in enumerate(final_target):\n",
        "            for pos_, j in enumerate(i):\n",
        "                if pos_ > 20:\n",
        "                    print(i)\n",
        "                if pos_ > 0:\n",
        "                    outs[pos, pos_-1, j] = 1 # one-hot\n",
        "#             if pos%1000 == 0 :\n",
        "#                 print('{} entries completed'.format(pos)) # format()填充{}，格式化输出\n",
        "#         print('\\nouts.shape: ', outs.shape)\n",
        "        final_target = sequence.pad_sequences(final_target, maxlen=maxLen,\n",
        "                                        dtype='int32', padding='post', \n",
        "                                         truncating='post')\n",
        "        context = sequence.pad_sequences(context, maxlen=maxLen,\n",
        "                                   dtype='int32', padding='post',\n",
        "                                   truncating='post')\n",
        "        yield ([context, final_target], outs)\n",
        "        steps += batch_size\n",
        "        if steps == 90000:\n",
        "            steps = 0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "K4gl3z7Tyczw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "def generate_test(batch_size):\n",
        "    print('\\n*********************************generate_test()*********************************')\n",
        "    steps=0\n",
        "    context_ = np.load(main_path + 'middle_data/context_indexes.npy')\n",
        "    final_target_ = np.load(main_path + 'middle_data/target_indexes.npy')\n",
        "    context_ = context_[90000:]\n",
        "    final_target_ = final_target_[90000:]\n",
        "    while True:\n",
        "        context = context_[steps:steps+batch_size]\n",
        "        final_target = final_target_[steps:steps+batch_size]\n",
        "        for pos, i in enumerate(final_target):\n",
        "            for pos_, j in enumerate(i):\n",
        "                i[pos_] = j + 1\n",
        "            if(len(i) > maxLen):\n",
        "                final_target[pos] = i[:maxLen]\n",
        "\n",
        "        for pos, i in enumerate(context):\n",
        "            for pos_, j in enumerate(i):\n",
        "                i[pos_] = j + 1\n",
        "            if(len(i) > maxLen):\n",
        "                context[pos] = i[:maxLen]\n",
        "#         print('\\ncontext.shape: ', context.shape)\n",
        "        outs = np.zeros([context.shape[0], maxLen, vocab_size], dtype='float32')\n",
        "        for pos, i in enumerate(final_target):\n",
        "            for pos_, j in enumerate(i):\n",
        "                if pos_ > 20:\n",
        "                    print(i)\n",
        "                if pos_ > 0:\n",
        "                    outs[pos, pos_-1, j] = 1 # one-hot\n",
        "#             if pos%1000 == 0 :\n",
        "#                 print('{} entries completed'.format(pos)) # format()填充{}，格式化输出\n",
        "#         print('\\nouts.shape: ', outs.shape)\n",
        "        final_target = sequence.pad_sequences(final_target, maxlen=maxLen,\n",
        "                                        dtype='int32', padding='post', \n",
        "                                         truncating='post')\n",
        "        context = sequence.pad_sequences(context, maxlen=maxLen,\n",
        "                                   dtype='int32', padding='post',\n",
        "                                   truncating='post')\n",
        "        yield ([context, final_target], outs)\n",
        "        steps += batch_size\n",
        "        if steps == 10000:\n",
        "            steps = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KyXaCfyZo-ch",
        "colab_type": "code",
        "outputId": "08e1dc0e-f53f-4b11-f183-ba896a5f41de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Dense, LSTM, TimeDistributed, Bidirectional, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "embed_layer = Embedding(input_dim=vocab_size, output_dim=50, trainable=True)\n",
        "embed_layer.build((None,))\n",
        "embed_layer.set_weights([embedding_matrix])\n",
        "\n",
        "LSTM_cell = Bidirectional(LSTM(512, return_sequences=True, return_state=True))\n",
        "LSTM_decoder = LSTM(1024, return_sequences=True, return_state=True)\n",
        "\n",
        "dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "  #encoder输入 与 decoder输入\n",
        "input_context = Input(shape=(None, ), dtype='int32', name='input_context')\n",
        "input_target = Input(shape=(None, ), dtype='int32', name='input_target')\n",
        "\n",
        "input_context_embed = embed_layer(input_context)\n",
        "input_target_embed = embed_layer(input_target)\n",
        "\n",
        "encoder_out, forward_h, forward_c, backward_h, backward_c = LSTM_cell(input_context_embed)\n",
        "context_h = Concatenate()([forward_h, backward_h])\n",
        "context_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "\n",
        "decoder_lstm, _, _ = LSTM_decoder(input_target_embed, \n",
        "                                    initial_state=[context_h, context_c])\n",
        "\n",
        "output = dense(decoder_lstm)\n",
        "\n",
        "model = Model([input_context, input_target], output)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "#   model.summary() \n",
        "\n",
        "filepath = main_path + \"modles/W2-\" + \"-{epoch:03d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                                 monitor='loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=False,\n",
        "                                 mode='min',\n",
        "                                 period=1,\n",
        "                                 save_weights_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "  \n",
        "initial_epoch=0\n",
        "file_list = os.listdir(main_path + 'modles/')\n",
        "if len(file_list) > 0:\n",
        "  epoch_list = get_file_list(main_path + 'modles/')\n",
        "  epoch_last = epoch_list[-1]\n",
        "  model.load_weights(main_path + 'modles/' + epoch_last)\n",
        "#     if len(file_list) > 2:\n",
        "#         for file_name in file_list[:-2]:\n",
        "#             file_ = main_path + 'modles/' + file_name\n",
        "#             os.remove(file_)\n",
        "#             print('Removed Successful! -- ', file_name)\n",
        "  print(\"**********checkpoint_loaded: \", epoch_last)\n",
        "  initial_epoch = int(epoch_last.split('-')[2]) + 1\n",
        "  print('**********Begin from epoch: ', str(initial_epoch))\n",
        "    \n",
        "\n",
        "model.fit_generator(generate_train(batch_size=100), \n",
        "              steps_per_epoch=900, # (total samples) / batch_size 90000/100 = 900\n",
        "              epochs=100, \n",
        "              verbose=1, \n",
        "              callbacks=callbacks_list, \n",
        "              validation_data=generate_test(batch_size=100), \n",
        "              validation_steps=100, # 10000/100 = 100\n",
        "              class_weight=None, \n",
        "              max_queue_size=10, \n",
        "              workers=1, \n",
        "              use_multiprocessing=False, \n",
        "              shuffle=True, \n",
        "              initial_epoch=initial_epoch\n",
        "             )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********checkpoint_loaded:  W2--011-1.9800-bigger.hdf5\n",
            "**********Begin from epoch:  12\n",
            "Epoch 13/100\n",
            "\n",
            "*********************************generate_test()*********************************\n",
            "\n",
            "*********************************generate_arrays_from_file()*********************************\n",
            "592/900 [==================>...........] - ETA: 4:09 - loss: 1.9257 - acc: 0.1410"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gmiOjkRzLGI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x95onNF_JGKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "8a9718e2-a124-44ad-99fe-5f3a471aa578"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/modles/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "W2--001-2.0121-bigger.hdf5  W2--007-1.9826-bigger.hdf5\n",
            "W2--002-2.0082-bigger.hdf5  W2--008-1.9802-bigger.hdf5\n",
            "W2--003-1.9931-bigger.hdf5  W2--009-1.9799-bigger.hdf5\n",
            "W2--004-1.9882-bigger.hdf5  W2--010-1.9808-bigger.hdf5\n",
            "W2--005-1.9818-bigger.hdf5  W2--011-1.9800-bigger.hdf5\n",
            "W2--006-1.9815-bigger.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}